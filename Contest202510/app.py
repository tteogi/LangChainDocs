import streamlit as st
from dotenv import load_dotenv
import os
import time
from document_loader import load_documents
from vector_store import VectorStoreManager

load_dotenv()

APP_TITLE = os.getenv("APP_TITLE", "ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì±—ë´‡")

st.set_page_config(page_title=APP_TITLE, page_icon="ğŸ¤–", layout="wide")

header_container = st.container()

with header_container:
    st.markdown(
        f"""
        <style>
        .stContainer {{
            position: sticky;
            top: 0;
            background-color: white;
            z-index: 999;
            padding-top: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 2px solid #f0f2f6;
        }}
        </style>
        """,
        unsafe_allow_html=True
    )
    st.title(f"ğŸ¤– {APP_TITLE}")

if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.welcome_shown = False

if "vectorstore_manager" not in st.session_state:
    st.session_state.vectorstore_manager = VectorStoreManager()

if "documents_loaded" not in st.session_state:
    st.session_state.documents_loaded = False

with st.sidebar:
    st.title(f"ğŸ¤– {APP_TITLE}")
    st.divider()
    st.header("ì„¤ì •")

    model_type = st.selectbox("ëª¨ë¸ ìœ í˜•", ["OpenAI", "Ollama"])

    if model_type == "OpenAI":
        openai_api_key = st.text_input(
            "OpenAI API Key", type="password", value=os.getenv("OPENAI_API_KEY", "")
        )
        model_name = st.selectbox("ëª¨ë¸ ì„ íƒ", ["gpt-5", "gpt-5-mini"])
    else:
        openai_api_key = None
        model_name = st.text_input("Ollama ëª¨ë¸ ì´ë¦„", value="llama2")
        st.info("Ollamaê°€ ë¡œì»¬ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")

    st.divider()

    st.header("ë¬¸ì„œ ì—…ë¡œë“œ")
    uploaded_files = st.file_uploader(
        "PDF, PowerPoint ë˜ëŠ” Markdown íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
        type=["pdf", "pptx", "md", "markdown"],
        accept_multiple_files=True,
    )

    if st.button("ë¬¸ì„œ ì²˜ë¦¬", type="primary"):
        if uploaded_files:
            if model_type == "OpenAI" and not openai_api_key:
                st.error("OpenAI API í‚¤ë¥¼ ë¨¼ì € ì…ë ¥í•´ì£¼ì„¸ìš”! ğŸ”‘")
            else:
                with st.spinner("ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ëŠ” ì¤‘..."):
                    try:
                        documents = load_documents(uploaded_files)

                        if documents:
                            st.session_state.vectorstore_manager.create_vectorstore(
                                documents, openai_api_key
                            )
                            st.session_state.documents_loaded = True
                            st.session_state.messages = []
                            st.success(
                                f"ì¢‹ì•„ìš”! ğŸ“š {len(documents)}ê°œì˜ ë¬¸ì„œ ì²­í¬ê°€ ì¤€ë¹„ë˜ì—ˆì–´ìš”. ì´ì œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”!"
                            )
                        else:
                            st.error("ë¬¸ì„œë¥¼ ì½ì„ ìˆ˜ ì—†ì–´ìš”. íŒŒì¼ í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”! ğŸ“„")
                    except Exception as e:
                        st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”: {str(e)}")
        else:
            st.warning("íŒŒì¼ì„ ë¨¼ì € ì—…ë¡œë“œí•´ì£¼ì„¸ìš”! ğŸ“")

    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.messages = []
        st.rerun()

if len(st.session_state.messages) == 0 and st.session_state.documents_loaded:
    welcome_msg = "ì•ˆë…•í•˜ì„¸ìš”! ğŸ‘‹ ë¬¸ì„œê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. ê¶ê¸ˆí•œ ì ì„ í¸í•˜ê²Œ ë¬¼ì–´ë³´ì„¸ìš”!"
    with st.chat_message("assistant"):
        st.markdown(welcome_msg)
    st.session_state.messages.append({"role": "assistant", "content": welcome_msg})

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        if not st.session_state.documents_loaded:
            response = "ì•—, ì•„ì§ ë¬¸ì„œê°€ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ì–´ìš”! ğŸ˜Š ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ë¬¸ì„œë¥¼ ë¨¼ì € ì—…ë¡œë“œí•´ì£¼ì‹œë©´ ì§ˆë¬¸ì— ë‹µë³€í•´ë“œë¦´ê²Œìš”."
            st.markdown(response)
        else:
            if model_type == "OpenAI" and not openai_api_key:
                response = "OpenAI API í‚¤ê°€ í•„ìš”í•´ìš”. ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”! ğŸ”‘"
                st.markdown(response)
            else:
                try:
                    qa_chain = st.session_state.vectorstore_manager.get_qa_chain(
                        model_name, model_type, openai_api_key
                    )

                    if qa_chain:
                        start_time = time.time()
                        
                        with st.status("ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆì–´ìš”... ğŸ¤”", expanded=True) as status:
                            st.write("ğŸ“„ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
                            search_start = time.time()
                            
                            result = qa_chain.invoke({"query": prompt})
                            response = result["result"]
                            
                            end_time = time.time()
                            total_time = end_time - start_time
                            
                            status.update(
                                label=f"ë‹µë³€ ì™„ë£Œ! âœ¨ (ì†Œìš”ì‹œê°„: {total_time:.2f}ì´ˆ)", 
                                state="complete", 
                                expanded=False
                            )

                        if (
                            not response
                            or response.strip() == ""
                            or "NOT_FOUND" in response
                        ):
                            response = "ì£„ì†¡í•´ìš”, ì—…ë¡œë“œí•˜ì‹  ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”. ğŸ˜… ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ë³´ì‹œê±°ë‚˜, ë” ìì„¸í•œ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ ë‹´ë‹¹ì ìœ¤ë•ì—´ë‹˜ê»˜ ì—°ë½í•´ì£¼ì„¸ìš”!"
                            st.markdown(response)
                        else:
                            st.markdown(response)
                            
                            st.caption(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")

                            if result.get("source_documents"):
                                with st.expander("ì°¸ì¡° ë¬¸ì„œ"):
                                    for i, doc in enumerate(result["source_documents"]):
                                        source_file = doc.metadata.get("source", "ì•Œ ìˆ˜ ì—†ìŒ")
                                        page = doc.metadata.get("page", "")
                                        st.markdown(f"**ì¶œì²˜ {i+1}:** {source_file}" + (f" (í˜ì´ì§€ {page + 1})" if page != "" else ""))
                                        st.markdown(doc.page_content[:300] + "...")
                    else:
                        response = "ë¬¸ì„œ ì²˜ë¦¬ì— ë¬¸ì œê°€ ìˆëŠ” ê²ƒ ê°™ì•„ìš”. ë¬¸ì„œë¥¼ ë‹¤ì‹œ ì—…ë¡œë“œí•´ë³´ì‹œê² ì–´ìš”?"
                        st.markdown(response)
                except Exception as e:
                    response = f"ì•—, ì²˜ë¦¬ ì¤‘ì— ë¬¸ì œê°€ ë°œìƒí–ˆì–´ìš”. ğŸ˜… ì§ˆë¬¸ì„ ë‹¤ì‹œ í•´ë³´ì‹œê±°ë‚˜ ë‹´ë‹¹ì ìœ¤ë•ì—´ë‹˜ê»˜ ë¬¸ì˜í•´ì£¼ì„¸ìš”!"
                    st.markdown(response)
                    st.error(f"ì˜¤ë¥˜: {str(e)}")

        st.session_state.messages.append({"role": "assistant", "content": response})
